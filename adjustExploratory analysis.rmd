---
title: "project-583"
author: "Lili Tang: Zhijia Ju"
output: pdf_document
---

```{r, include = FALSE}
library(ggplot2)
library(gridExtra)
library(glmnet)
library(np)
library(nortest)
library(MASS)
library(glmnet)
library(leaps)
```
```{r results="hide"}
data <- read.csv("data_cleaned.csv", header = TRUE, check.names = FALSE)
dim(data) # 189, 31
data_no_date <- data[, -1]
head(data)
```

**We aim to predict the price of ethylene glycol using the following 29 explanatory variables.**
MEG refers to ethylene glycol.     
**y	MEG spot price**       

**Up-stream price of MEG**            
x1	WTI: West Texas Intermediate(Crude Oil)Price               
x2	Brent: Brent Crude oil price                
x3	Coal price       

**Up-stream(Ethylene)Profit**             
x4	domestic;  x5	foreign                                     

**MEG Profit**                
x6	made of coal;  x7	made of Ethylene                                       

**MEG operating rate**                
x8	domestic;  x9	foreign                                   

**Downstream profits**                                           
x10	Recycled Bottle Chips;  x11	polyester chips;  x12	polyester bottle chip;  x13	POY;  x14	FDY;  x15	DTY;  x16	Polyester                            
**Downstream operating rate**                 
x17	Polyester; x18	filament; x19	Direct spinning;  x20	chip spun filament;  x21	texturing machine;  x22	weaving machine    

**Downstream Inventory**                   
x24	Polyester; x25	FDY; x26	DTY;  x27	POY                 

**MEG Inventory**                    
x28	foreign Port;  x29	domestic Port;  x30	factory                          

## 1. A statistically descriptive analysis of the dataset
The data structure is shown below. We can see that all the variables are continuous values.
```{r}
str(data)
```
Here is the summary statistics of the response variable.
```{r}
summary(data$y)
```
Let's explore the histogram of the response variable y and some of the explanatory variables.

```{r, warning = FALSE, message = FALSE}
# Create histograms of some of the variables
hist_y <- ggplot(data, aes(x = y)) + geom_histogram() + labs(title = "Histogram of y")
hist_x1 <- ggplot(data, aes(x = x1)) + geom_histogram() + labs(title = "Histogram of x1")
hist_x11 <- ggplot(data, aes(x = x11)) + geom_histogram() + labs(title = "Histogram of x11")
hist_x21 <- ggplot(data, aes(x = x21)) + geom_histogram() + labs(title = "Histogram of x21")
grid.arrange(hist_y, hist_x1, hist_x11, hist_x21, nrow = 2, ncol = 2)
```
The histogram of the response variable y seems to have 3 peaks. The histogram of the explanatory variable x1 and x11 are right skewed. The histogram of the explanatory variable x21 is left skewed.

Let's explore the scatterplots of the response variable y and some of the explanatory variables.
```{r}
# create scatterplots of the variables x1, x2, x3, x4, x5, x7, x10, x11, x12
scatter_x1 <- ggplot(data = data, aes(x = x1, y = y)) + geom_point() + labs(title = "Scatterplot of x1 and y")
scatter_x7 <- ggplot(data = data, aes(x = x7, y = y)) + geom_point() + labs(title = "Scatterplot of x7 and y")
scatter_x14 <- ggplot(data = data, aes(x = x14, y = y)) + geom_point() + labs(title = "Scatterplot of x14 and y")
scatter_x30 <- ggplot(data = data, aes(x = x30, y = y)) + geom_point() + labs(title = "Scatterplot of x30 and y")
grid.arrange(scatter_x1, scatter_x7, scatter_x14, scatter_x30, ncol = 2, nrow = 2)
```
There seems to have a positive linear relationship between the response variable y and the explanatory variables x1, x7. And a negative relationship between y and x30. It seems reasonable because x1 represent Crude Oil Price, which is positively related to the price of y(MEG), while x30 represents the factory inventory, the higher the inventory, the lower the price.
There is no obvious relationship between y and x14.

Let's explore the trend of the response variable y and some of the explanatory variables over time.
```{r}
# plot the variables versus date
trend_y <- ggplot(data = data, aes(x = date, y = y)) + geom_point() + labs(title = "Trend of y over time")
trend_x1 <- ggplot(data = data, aes(x = date, y = x1)) + geom_point() + labs(title = "Trend of x1 over time")
trend_x4 <- ggplot(data = data, aes(x = date, y = x4)) + geom_point() + labs(title = "Trend of x4 over time")
trend_x11 <- ggplot(data = data, aes(x = date, y = x11)) + geom_point() + labs(title = "Trend of x11 over time")
grid.arrange(trend_y, trend_x1, trend_x4, trend_x11, ncol = 2, nrow = 2)
```
The trend plots of y and x1 seem to have some similarities. The trend plots of x4 and x11 have no obvious trend.


## 2. Applications of statistical analysis techniques

### kernel density

Let's try to plot a kernel density estimate for y using an Epanechnikov kernel.
```{r}
bw <- npudensbw( ~ y, data = data, ckertype = "epanechnikov", bwmethod = "cv.ml")
fhat <- npudens(bws = bw)
plot(fhat)
```

As stated in the histogram part, the distribution of y is not normal and it seems to have 3 peaks.

Let's conduct Pearson tests for normality.
```{r}
pearson.test(data$y)
pearson.test(data$x11)
```
From the pearson chi-square normality tests, we can see that the p-value is very small for y, and large for x11.
So we have evidence that y is not normally distributed, but x11 is normally distributed.

### Linear regression
Let's try a linear regression model to do diagnostics.
```{r}
# fit a linear regression model
model_ls <- lm(y ~ ., data = data_no_date)
summary(model_ls)
```

From the summary output, the can see that only variables x3, x6, x7, x13, x14, x20, x22, x25, x27, x28 and x30 are significant.
The R-squared and adjusted R-square are high, which is expected as we have lots of variables. Variable selection is needed to reduce the number of variables.

Let's try to do variable selection using stepwise regression.
```{r}
model_step <- stepAIC(model_ls, direction = "both", criterion = "bic", trace = FALSE)
model_step
```
From the stepwise model selection, 6 variables are removed from the model.

```{r}
par(mfrow=c(2,2))
plot(model_step)
```

### Lasso

Let's try to do variable selection using lasso regression.
```{r}
# fit a lasso regression model
model_lasso <- cv.glmnet(x = as.matrix(data_no_date[, -1]), y = data_no_date[, 1], alpha = 1)
# plot the lasso regression model
plot(model_lasso)
```

Both minimum and 1se lines suggest to keep 23 variables, which is the same as the stepwise model selection.


## 3. Scientific questions  4. Statistical analysis techniques I will use to answer those questions (with justification)

**Q1. Based no our dataset, which models can be use to forecast the price trend of ethylene glycol?**

From above analysis, linear regression model seems good in interpretation. R-square is 0.99 and many predictors are significant. However, as reponse variable is not normal distributed, it may violate the assumptions of linear regression and lead to biased or unreliable results. Therefore, we can use other methods such **nonparametric local linear regression model or regression trees or random forest**.

We also can consider using **logistic regression**, but we need to convert the continuous response variable into a binary outcome by applying a threshold value. For example, if the response variable increases compared with last day or last week, then we assign it to 1, otherwise, we assign it to 0. 

### Nonparametric local linear regression model

```{r}
library(np)
bw <- npregbw(y ~ x1 + x2 + x3 + x4 + x6 + x7 + x9 + x10 + x11 + 
   x13 + x14 + x15 + x16 + x19 + x20 + x21 + x22 + x24 + x25 + 
   x26 + x27 + x28 + x30,data=data_no_date,regtype="ll")
model.ll <- npreg(bws=bw)
summary(model.ll)
```

In nonparametric local linear regression model, R square is quite close to 1(larger than R square in linear regression), which indicates that the model is fitting the data very well, and that a high proportion of the variability in the data is being accounted for by the model.

### Logistic 

```{r, warning = FALSE, message = FALSE}
data_logistic <- data_no_date[-1,]
data_logistic$newy <- ifelse(diff(data_no_date$y) > 0, 1, 0)
data_logistic$newy <- as.factor(data_logistic$newy)
model_Log<- glm(newy ~., data=data_logistic[,-1],family =binomial)
model_Log
model_Log_step <- stepAIC(model_Log, direction = "both", criterion = "bic", trace = FALSE)
model_Log_step
```

By variable selection, AIC decreases and AIC value of 206.2 suggests that the logistic regression model fits the data well and has good predictive power.


**Q2. There are 29 variables in our dataset, how can we reduce the dimension and avoid multicollinearity?**

There are 2 ways we can consider, one way is lasso, which can be used to reduce high-dimensional data in a model by shrinking the coefficients of irrelevant features to zero.
Another way is PCA, which aims to reduce the dimensionality of the data while retaining as much of its variance as possible

### PCA
```{r results="hide"}
pca <- prcomp(data_no_date[,-c(1)], scale.=TRUE)
summary(pca)
```
```{r}
var_explained = cumsum(pca $sdev^2 / sum(pca $sdev^2))
# Determine the number of components needed to retain at least 90% of the variance
which.max(var_explained >= 0.9)
plot(pca, type="lines", npcs=131)
```

By using PCA, we can reduce 29 variables to 11 variables(those can explain 90% of the model)



**Q3. which model is the most appropriate and accurate?**

First of all, it depends on our purpose. If our priority is to interpret, linear regression, logistic regression or trees will be better. This involves examining the coefficients of the model to determine the direction and strength of the relationships between the variables. This information can be used to identify the most important predictors and to generate hypotheses about the underlying mechanisms driving the relationship between the variables.

If we focus more on predication, we can divide our data into training and testing, using cross validation to test the model with the smallest test MSE. Normally, random forest might be a good choice.

